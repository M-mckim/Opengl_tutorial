{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERP image -> 6 tiles perspective image -> 6 tiles VMAF sacore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#### SR 관련 라이브러리 ####\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Conv2d, Sequential, Parameter\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import argparse\n",
    "import sys\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import subprocess\n",
    "\n",
    "from collections.abc import Iterator\n",
    "\n",
    "from architect.model_factory.torch.owl_model_v1 import GeneratedSingleExitModelY\n",
    "\n",
    "from custom_types.types import UpsampleType, BlockType, BlockInferenceType\n",
    "from model.omnisrnet.modules.torch import SubPixelUpscaleModule, InformationMultiDistillationBlock, DenseBlock, ResidualBlock, \\\n",
    "    ProgressiveUpScaleModule, ChannelAttentionBlock\n",
    "from model.omnisrnet.modules.torch import SubPixelUpscaleModuleY, ProgressiveUpScaleModuleY\n",
    "from model.omnisrnet.modules.torch.function import initializer\n",
    "#### OpenGL 관련 라이브러리 ####\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "from OpenGL import EGL\n",
    "from OpenGL.GLES3 import *\n",
    "import ctypes\n",
    "from PIL import Image\n",
    "import pyrr\n",
    "import cv2 as cv\n",
    "import time\n",
    "\n",
    "from sphere import Sphere\n",
    "from camera import Camera\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['PATH'] = '/data/home/mckim/anaconda3/envs/Eagle/bin:' + env['PATH']\n",
    "env['LD_LIBRARY_PATH'] = '/data/home/mckim/anaconda3/envs/Eagle/lib/x86_64-linux-gnu:' + env['LD_LIBRARY_PATH']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VMAF 측정 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_vmaf(ref, dis):\n",
    "    cmd = [\n",
    "        'ffmpeg', '-hide_banner', '-loglevel', 'error',\n",
    "        '-i', dis, # distorted\n",
    "        '-i', ref, # reference\n",
    "        '-filter_complex', f'[0:v]scale=1280x1280:flags=bicubic[distorted];[distorted][1:v]libvmaf=feature=name=psnr|name=float_ssim:model=path=/data/home/mckim/Eagle/VMAF-Tester/vmaf/model/vmaf_v0.6.1.json\\\\\\\\:enable_transform=true:log_fmt=json:log_path=/dev/stdout:n_threads=6',\n",
    "        # '-filter_complex', '[0:v]scale=1280x720:flags=bicubic[distorted];[distorted][1:v]libvmaf=feature=name=psnr|name=float_ssim:model=path=/data/home/mckim/Eagle/VMAF-Tester/vmaf/model/vmaf_v0.6.1.json\\\\\\\\:enable_transform=true:log_fmt=json:log_path=/dev/stdout:n_threads=6',\n",
    "        '-f', 'null', '-'\n",
    "        ]\n",
    "    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True, env=env)\n",
    "    vmaf_result = json.loads(result.stdout)\n",
    "    return {\n",
    "        'reference':ref,\n",
    "        'distorted':dis,\n",
    "        'vmaf_mean':vmaf_result['pooled_metrics']['vmaf']['mean'],\n",
    "        'vmaf_harmonic_mean':vmaf_result['pooled_metrics']['vmaf']['harmonic_mean'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': '/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/first_frame_1280x720_BICUBIC_PIL.png',\n",
       " 'distorted': '/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/first_frame_1280x720_AREA_CV.png',\n",
       " 'vmaf_mean': 100.0,\n",
       " 'vmaf_harmonic_mean': 100.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = '/data/home/mckim/Eagle/360viwer_py/result_img/test_sphere_view_LR_(90,45).png'\n",
    "dis = '/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/first_frame_1280x720_AREA_CV.png'\n",
    "ref = '/data/home/mckim/Eagle/360viwer_py/result_img/test_sphere_view_(90,45).png'\n",
    "ref = '/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/first_frame_1280x720_BICUBIC_PIL.png'\n",
    "json_result = calculate_vmaf(ref, dis)\n",
    "json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result['vmaf_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지 SR module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_block_v5(block, input_channels) -> Sequential:\n",
    "    block_type, block_info = block\n",
    "    results = [None]\n",
    "    if block_type == BlockType.IMD:\n",
    "        results[-1] = InformationMultiDistillationBlock(\n",
    "            layer_depth=block_info['layer_depth'],\n",
    "            input_channels=input_channels,\n",
    "            bottleneck_channels=block_info.get('bottleneck_channel'),\n",
    "        )\n",
    "    elif block_type == BlockType.Residual:\n",
    "        results[-1] = ResidualBlock(\n",
    "            layer_depth=block_info['layer_depth'],\n",
    "            input_channels=input_channels,\n",
    "            bottleneck_channels=block_info.get('bottleneck_channel'),\n",
    "        )\n",
    "    elif block_type == BlockType.Dense:\n",
    "        results[-1] = DenseBlock(\n",
    "            input_channels=input_channels,\n",
    "            bottleneck_channels=block_info.get('bottleneck_channel'),\n",
    "            layer_depth=block_info['layer_depth'],\n",
    "        )\n",
    "    ca = block_info['channel_attention']\n",
    "    if ca > 0:\n",
    "        results.append(ChannelAttentionBlock(reduction=ca, input_channels=input_channels))\n",
    "    return Sequential(*results)\n",
    "\n",
    "# Multi-Exit -> Single-Exit -> YUV에서 Y-only!\n",
    "class GeneratedSingleExitModelY(Module):\n",
    "    def __init__(self, arch):\n",
    "        super(GeneratedSingleExitModelY, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.scale_factor = arch.get('scale_factor') # or 4\n",
    "        self.hidden_channels = arch['block_channels']\n",
    "        block_channels = arch['block_channels']\n",
    "        self.block_length = len(arch['block'][1].keys()) + 1\n",
    "\n",
    "        # Feature Extractor 0 (Input layer)\n",
    "        self.blocks0 = Conv2d(\n",
    "            in_channels=1, # 3 -> 1 (RGB -> Y)\n",
    "            out_channels=block_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding='same'\n",
    "        )\n",
    "        initializer(self.blocks0)\n",
    "\n",
    "        # Feature Extractor 1 ~\n",
    "        for b_idx, b in arch['block'][1].items():\n",
    "            b_idx = int(b_idx)\n",
    "            self.__setattr__(f'blocks{b_idx + 1}', make_block_v5(b, block_channels))\n",
    "            if b[1]['block_inference_type'] == BlockInferenceType.FrameConjunction:\n",
    "                pre_c = Conv2d(\n",
    "                    in_channels=block_channels * 2,\n",
    "                    out_channels=block_channels,\n",
    "                    kernel_size=(3, 3),\n",
    "                    padding='same'\n",
    "                )\n",
    "                post_c = Conv2d(\n",
    "                    in_channels=block_channels * 2,\n",
    "                    out_channels=block_channels,\n",
    "                    kernel_size=(3, 3),\n",
    "                    padding='same'\n",
    "                )\n",
    "                initializer(pre_c)\n",
    "                initializer(post_c)\n",
    "                self.__setattr__(f'blocks{b_idx + 1}pre', pre_c)\n",
    "                self.__setattr__(f'blocks{b_idx + 1}post', post_c)\n",
    "\n",
    "        # Reconstructor for the final exit\n",
    "        if arch['upsampling_type'] == UpsampleType.Progressive:\n",
    "            self.upsample = ProgressiveUpScaleModuleY(\n",
    "                upsampling_channels=arch['upsampling_channels'],\n",
    "                scale_factor=self.scale_factor,\n",
    "                input_channel=block_channels\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = SubPixelUpscaleModuleY(\n",
    "                scale_factor=self.scale_factor,\n",
    "                input_channel=block_channels\n",
    "            )\n",
    "        # self.__setattr__(f'upsample{b_idx + 1}', u)\n",
    "        print(\"[DEBUG, owl_model_v1] Reconstructors\", self.upsample)\n",
    "\n",
    "\n",
    "    def frame_conjunction_params(self) -> Iterator[Parameter]:\n",
    "        for p in self.rgb.parameters():\n",
    "            yield p\n",
    "        for p in self.frame_conjunction.parameters():\n",
    "            yield p\n",
    "        for p in self.frame_conjunction_pre.parameters():\n",
    "            yield p\n",
    "        for p in self.frame_conjunction_post.parameters():\n",
    "            yield p\n",
    "\n",
    "    def feature_extractor_params(self, layer_idx=-1) -> Iterator[Parameter]:\n",
    "        if layer_idx == -1:\n",
    "            # ALL\n",
    "            for b in range(self.block_length):\n",
    "                for p in self.__getattr__(f'blocks{b}').parameters():\n",
    "                    yield p\n",
    "\n",
    "        # Specific exit\n",
    "        elif 0 < layer_idx < self.block_length:\n",
    "            for p in self.__getattr__(f'blocks{layer_idx}').parameters():\n",
    "                yield p\n",
    "\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def reconstructor_params(self) -> Iterator[Parameter]:\n",
    "        for p in self.upsample.parameters():\n",
    "            yield p\n",
    "\n",
    "    def exit_idx_params(self, idx) -> Iterator[Parameter]:\n",
    "        for p in self.__getattr__(f'upsample{idx}').parameters():\n",
    "            yield p\n",
    "        for i in range(idx + 1):\n",
    "            for p in self.__getattr__(f'blocks{i}').parameters():\n",
    "                yield p\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        B, T, C, H, W = x_input.size()\n",
    "        out = x_input.view(B*T, C, H, W)\n",
    "        \n",
    "        # srs = []\n",
    "\n",
    "        # biups\n",
    "        biups = F.interpolate(out, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)    # B*T C sH sW\n",
    "\n",
    "        # Block 0~ + Exit 0~\n",
    "        for b_idx in range(self.block_length):\n",
    "            if not hasattr(self, f'blocks{b_idx}pre'):\n",
    "                out2 = self.__getattr__(f'blocks{b_idx}')(out)\n",
    "            else:\n",
    "                out2 = out.view(B, T, -1, H, W)\n",
    "                forward_hiddens, backward_hiddens = [], []\n",
    "                for i in range(T):\n",
    "                    forward = out2[:, i]\n",
    "                    backward = out2[:, T - i - 1]\n",
    "                    outf = forward\n",
    "                    outb = backward\n",
    "                    if i > 0:\n",
    "                        outf = torch.cat([outf, forward_hiddens[-1]], dim=1)\n",
    "                        outb = torch.cat([outb, backward_hiddens[-1]], dim=1)\n",
    "                        outf = self.__getattr__(f'blocks{b_idx}pre')(outf)\n",
    "                        outb = self.__getattr__(f'blocks{b_idx}pre')(outb)\n",
    "                    outf = self.__getattr__(f'blocks{b_idx}')(outf) # + forward    # B (bc) H W\n",
    "                    outb = self.__getattr__(f'blocks{b_idx}')(outb) # + backward    # B (bc) H W\n",
    "                    forward_hiddens.append(outf)\n",
    "                    backward_hiddens.append(outb)\n",
    "                res = []\n",
    "                for i in range(T):\n",
    "                    forward = torch.cat([forward_hiddens[i], backward_hiddens[T - i - 1]], dim=1)    # B 2(bc) H W\n",
    "                    forward = self.__getattr__(f'blocks{b_idx}post')(forward)    # B (bc) H W\n",
    "                    res.append(forward)\n",
    "                out2 = torch.stack(res, dim=1)    # B T (bc) H W # Skip Connection\n",
    "                out2 = out2.view(B * T, -1, H, W)    # B*T (bc) H W\n",
    "            if b_idx > 0:\n",
    "                out = out2 + out\n",
    "            else:\n",
    "                out = out2\n",
    "\n",
    "#             print(\"[DEBUG, anytime_model_v5] (forward) feature extractors > b_idx, out.shape:\", b_idx, out.shape)\n",
    "#             print(\"[DEBUG, anytime_model_v5] B, T, C, H, W, self.scale_factor:\", B, T, C, H, W, self.scale_factor)\n",
    "#             # print(\"[DEBUG, anytime_model_v5], sH sW:\", B, T, C, self.scale_factor * H, self.scale_factor * W)\n",
    "            \n",
    "#             print(\"[DEBUG, anytime_model_v5] self.__getattr__(f'upsample{b_idx}')(out).shape:\", self.__getattr__(f'upsample{b_idx}')(out).shape)\n",
    "#             print(\"[DEBUG, anytime_model_v5] biups.shape:\", biups.shape)\n",
    "\n",
    "        # Upsample\n",
    "        # u = self.__getattr__(f'upsample{b_idx}')(out) + biups # B*T C sH sW\n",
    "        u = self.upsample(out) + biups # B*T C sH sW\n",
    "        u = torch.clamp(u, 0, 1) # tf.clip_by_value(u, 0, 1)\n",
    "        u = u.view(B, T, C, self.scale_factor * H, self.scale_factor * W)    # B T C sH sW\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[3, 4, 5]\n",
      "[5, 6, 7]\n",
      "[7, 8, 9]\n",
      "empty\n"
     ]
    }
   ],
   "source": [
    "frames = [[1,2,3],[3,4,5],[5,6,7],[7,8,9]]\n",
    "# check frames is empty\n",
    "while True:\n",
    "    if not frames:\n",
    "        print(\"empty\")\n",
    "        break\n",
    "    else:\n",
    "        print(frames.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr_init(exit_num):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"[DEBUG] device\", device)\n",
    "    \n",
    "    file_path = f'./model/owlnet/archs_x4.json'\n",
    "    with open(file_path, 'r') as f:\n",
    "        archs = json.load(f)\n",
    "        print(archs[int(exit_num)-1]) # [0] ~ [4]까지 있음 각각 exit1 ~ exit5\n",
    "    \n",
    "    # torch 모델 경로. 위에 archs[i] 중 하나를 골라서 넣어야 함\n",
    "    model_path = f'/data/home/mckim/Eagle/VMAF-Tester/Eagle_models/x4_Sub_12_exit{exit_num}_Dense_fixed_72_0_4_MODPerspective_general-01_ymodel_yuvdata_epoch10_hrdown1_lrdown4_maxframe9000.pt'\n",
    "    # dummy 모델 생성\n",
    "    model = GeneratedSingleExitModelY(archs[int(exit_num)-1]).cuda()\n",
    "    # torch 모델 로드\n",
    "    state_dict = torch.load(model_path)\n",
    "    # dummy 모델에 state_dict 로드\n",
    "    model.load_state_dict(state_dict)\n",
    "    # print(\"[DEBUG] model\", model)\n",
    "    \n",
    "    # 모델을 gpu로 올림\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "def inference(model, device, frames, sr_scale):\n",
    "    height, width = frames[0].shape\n",
    "    \n",
    "    # frames = np.zeros((len(frames), int(height *3/2), width), dtype=np.uint8)\n",
    "    # y_frames = np.zeros((len(frames), height , width), dtype=np.uint8)\n",
    "    sr_frame = np.zeros((int(height  *sr_scale), width *sr_scale), dtype=np.uint8)\n",
    "\n",
    "    y_frames = [frame[:height*2//3,:width] for frame in frames]\n",
    "    y_frames = np.array(y_frames)\n",
    "            \n",
    "    input_images = torch.tensor(y_frames, dtype=torch.uint8)\n",
    "    input_images = input_images.to(device)\n",
    "    input_images = input_images.float() / 255.0\n",
    "    # (B, 1, 1, H, W) 형태로 확장\n",
    "    input_images = input_images.unsqueeze(1).unsqueeze(1)  # 채널, 배치 차원 추가\n",
    "    # print(\"[DEBUG] input_image unsqueeze shape: \", input_images.shape)\n",
    "\n",
    "    # print(\"[DEBUG] preprocess timee: \", (time.time() - start_inf)* 1000) # 5ms\n",
    "    now = time.time()\n",
    "    with torch.no_grad(): # 빼면 output_of_memory 에러 발생! inference할 때는 with torch.no_grad()로 감싸줘야 함!\n",
    "        output_images = model(input_images)\n",
    "    # print(\"[DEBUG] output_image shape: \", output_images.shape)\n",
    "    print(\"[DEBUG] torch inference time: \", (time.time() - now)* 1000) # 25ms\n",
    "    now = time.time()\n",
    "    output_images = output_images.squeeze(1).squeeze(1)\n",
    "    output_images = (output_images *255).to(device, dtype=torch.uint8)\n",
    "    output_images = output_images.cpu().detach().numpy() # squeeze(0): 첫번째(index) 차원 제거, squeeze(): 크기가 1인 차원 제거\n",
    "    print(\"[DEBUG] cpu detach time\", (time.time() - now)* 1000) # 이게 왜 갑자기 오래걸리지?\n",
    "    now = time.time()\n",
    "    for i in range(len(frames)):\n",
    "        # output_images[i] *= 255\n",
    "        sr_frame = cv.resize(frames[i], (width *sr_scale, int(height *sr_scale)), interpolation=cv.INTER_NEAREST)\n",
    "        sr_frame[:height*2//3 *sr_scale,:width *sr_scale] = output_images[i]\n",
    "        \n",
    "        sr_frame = cv.cvtColor(sr_frame, cv.COLOR_YUV2BGR_I420)\n",
    "        cv.imwrite(f'Eagle_result/test_sphere_view_SR_({i}).png', sr_frame) ## 모든 delay는 저장이 원인. 이거 빼면 frame당 2ms 정도.\n",
    "        print(\"[DEBUG] video write time per frame: \", (time.time() - now)* 1000) # 80~120ms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERP -> 6 Tiles Opengl module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texture(texture, width, height):\n",
    "    glBindTexture(GL_TEXTURE_2D, texture)\n",
    "    # Set the texture wrapping parameters\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)\n",
    "    # Set texture filtering parameters\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n",
    "    \n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_BYTE, None)\n",
    "    return texture\n",
    "\n",
    "def compile_shader(source, shader_type):\n",
    "    shader = glCreateShader(shader_type)\n",
    "    glShaderSource(shader, source)\n",
    "    glCompileShader(shader)\n",
    "    \n",
    "    success = glGetShaderiv(shader, GL_COMPILE_STATUS)\n",
    "    if not success:\n",
    "        infoLog = glGetShaderInfoLog(shader)\n",
    "        print(f\"Shader compile error: {infoLog}\")\n",
    "    \n",
    "    return shader\n",
    "\n",
    "def create_program(vertex_src, fragment_src):\n",
    "    vertex_shader = compile_shader(vertex_src, GL_VERTEX_SHADER)\n",
    "    fragment_shader = compile_shader(fragment_src, GL_FRAGMENT_SHADER)\n",
    "    program = glCreateProgram()\n",
    "    glAttachShader(program, vertex_shader)\n",
    "    glAttachShader(program, fragment_shader)\n",
    "    glLinkProgram(program)\n",
    "    glDeleteShader(vertex_shader)\n",
    "    glDeleteShader(fragment_shader)\n",
    "    return program\n",
    "\n",
    "def create_opengl_context():\n",
    "    # 컨텍스트 생성 및 바인딩\n",
    "    config_attribs = [\n",
    "        EGL.EGL_SURFACE_TYPE, EGL.EGL_PBUFFER_BIT, \n",
    "        EGL.EGL_BLUE_SIZE, 8, \n",
    "        EGL.EGL_GREEN_SIZE, 8, \n",
    "        EGL.EGL_RED_SIZE, 8, \n",
    "        EGL.EGL_ALPHA_SIZE, 8,\n",
    "        EGL.EGL_RENDERABLE_TYPE, EGL.EGL_OPENGL_ES3_BIT, \n",
    "        EGL.EGL_NONE\n",
    "    ]\n",
    "    context_attribs = [\n",
    "        EGL.EGL_CONTEXT_CLIENT_VERSION, 3, \n",
    "        EGL.EGL_NONE\n",
    "    ]\n",
    "    # EGL 컨텍스트 설정 및 초기화\n",
    "    display = EGL.eglGetDisplay(EGL.EGL_DEFAULT_DISPLAY)\n",
    "    EGL.eglInitialize(display, None, None)\n",
    "    EGL.eglBindAPI(EGL.EGL_OPENGL_ES_API)\n",
    "    num_configs = EGL.EGLint()\n",
    "    config = EGL.EGLConfig()\n",
    "    EGL.eglChooseConfig(display, config_attribs, config, 1, num_configs)\n",
    "    egl_context = EGL.eglCreateContext(display, config, EGL.EGL_NO_CONTEXT, context_attribs)\n",
    "    EGL.eglMakeCurrent(display, EGL.EGL_NO_SURFACE, EGL.EGL_NO_SURFACE, egl_context)\n",
    "    return display, egl_context\n",
    "\n",
    "def view_change(program, view):\n",
    "    glUniformMatrix4fv(glGetUniformLocation(program, \"view\"), 1, GL_FALSE, view)\n",
    "\n",
    "def init_viewport(program, width, height):\n",
    "    glViewport(0, 0, width, height)\n",
    "    \n",
    "    projection = pyrr.matrix44.create_perspective_projection_matrix(90, width / height, 0.1, 100)\n",
    "    glUniformMatrix4fv(glGetUniformLocation(program, \"projection\"), 1, GL_FALSE, projection)\n",
    "\n",
    "    view = pyrr.matrix44.create_look_at(\n",
    "        pyrr.Vector3([0.0, 0.0, 0.0]), \n",
    "        pyrr.Vector3([0.0, 0.0, -1.0]), \n",
    "        pyrr.Vector3([0.0, 1.0, 0.0])\n",
    "    )\n",
    "    glUniformMatrix4fv(glGetUniformLocation(program, \"view\"), 1, GL_FALSE, view)\n",
    "\n",
    "    model = pyrr.matrix44.create_identity(dtype=np.float32)\n",
    "    glUniformMatrix4fv(glGetUniformLocation(program, \"model\"), 1, GL_FALSE, model)\n",
    "\n",
    "\n",
    "def destroy_opengl_context(display, egl_context):\n",
    "    EGL.eglDestroyContext(display, egl_context)\n",
    "    EGL.eglTerminate(display)\n",
    "\n",
    "def opengl_init(sphere, view_width, view_height, erp_width, erp_height):\n",
    "    \n",
    "    vertex_src = \"\"\"\n",
    "    attribute vec4 a_position;\n",
    "    attribute vec2 a_texCoord;\n",
    "\n",
    "    varying vec2 v_texCoord;\n",
    "\n",
    "    uniform mat4 projection;\n",
    "    uniform mat4 view;\n",
    "    uniform mat4 model;\n",
    "    \n",
    "    void main() {\n",
    "        gl_Position = projection * view * model * a_position;\n",
    "        v_texCoord = a_texCoord;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    fragment_src = \"\"\"\n",
    "    precision mediump float;\n",
    "    varying vec2 v_texCoord;\n",
    "    uniform sampler2D texture;\n",
    "    void main() {\n",
    "       gl_FragColor = texture2D(texture, v_texCoord);\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    program = create_program(vertex_src, fragment_src)\n",
    "    glUseProgram(program)\n",
    "\n",
    "    VAO = glGenVertexArrays(1)\n",
    "    glBindVertexArray(VAO)\n",
    "\n",
    "    VBO = glGenBuffers(1)\n",
    "    glBindBuffer(GL_ARRAY_BUFFER, VBO)\n",
    "    glBufferData(GL_ARRAY_BUFFER, sphere.vertices.nbytes, sphere.vertices, GL_STATIC_DRAW)\n",
    "\n",
    "    posotion_loc = glGetAttribLocation(program, \"a_position\")\n",
    "    glEnableVertexAttribArray(posotion_loc)\n",
    "    glVertexAttribPointer(posotion_loc, 3, GL_FLOAT, GL_FALSE, 12, ctypes.c_void_p(0))\n",
    "\n",
    "    texCoodBuffer = glGenBuffers(1)\n",
    "    glBindBuffer(GL_ARRAY_BUFFER, texCoodBuffer)\n",
    "    glBufferData(GL_ARRAY_BUFFER, sphere.texCoords.nbytes, sphere.texCoords, GL_STATIC_DRAW)\n",
    "\n",
    "    texCoord_loc = glGetAttribLocation(program, \"a_texCoord\")\n",
    "    glEnableVertexAttribArray(texCoord_loc)\n",
    "    glVertexAttribPointer(texCoord_loc, 2, GL_FLOAT, GL_FALSE, 0, ctypes.c_void_p(0))\n",
    "    \n",
    "    EVO = glGenBuffers(1)\n",
    "    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EVO)\n",
    "    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sphere.indexCount*8, sphere.indices, GL_STATIC_DRAW)\n",
    "    \n",
    "    # texture load\n",
    "    sphereTexture = glGenTextures(1)\n",
    "    load_texture(sphereTexture, erp_width, erp_height) # HR texture\n",
    "    \n",
    "    # texture load\n",
    "    sphereLRTexture = glGenTextures(1)\n",
    "    load_texture(sphereLRTexture, erp_width//4, erp_height//4) # LR texture: HR의 1/4 크기\n",
    "\n",
    "    framebuffer = glGenFramebuffers(1)\n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, framebuffer)\n",
    "\n",
    "    renderbuffer = glGenRenderbuffers(1)\n",
    "    glBindRenderbuffer(GL_RENDERBUFFER, renderbuffer)\n",
    "\n",
    "    # 렌더버퍼 생성 및 프레임버퍼에 연결\n",
    "    fbo_texture = glGenTextures(1)\n",
    "    glBindTexture(GL_TEXTURE_2D, fbo_texture)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, view_width, view_height, 0, GL_RGB, GL_UNSIGNED_BYTE, None)\n",
    "    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, fbo_texture, 0);  \n",
    "    # 렌더버퍼 생성 및 프레임버퍼에 연결\n",
    "    glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, view_width, view_height)\n",
    "    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, renderbuffer)\n",
    "    \n",
    "    lrframebuffer = glGenFramebuffers(1)\n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, lrframebuffer)\n",
    "\n",
    "    lrrenderbuffer = glGenRenderbuffers(1)\n",
    "    glBindRenderbuffer(GL_RENDERBUFFER, lrrenderbuffer)\n",
    "\n",
    "    # 렌더버퍼 생성 및 프레임버퍼에 연결\n",
    "    lrfbo_texture = glGenTextures(1)\n",
    "    glBindTexture(GL_TEXTURE_2D, lrfbo_texture)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, view_width//4, view_height//4, 0, GL_RGB, GL_UNSIGNED_BYTE, None)\n",
    "    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, lrfbo_texture, 0);  \n",
    "    # 렌더버퍼 생성 및 프레임버퍼에 연결\n",
    "    glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, view_width//4, view_height//4)\n",
    "    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, lrrenderbuffer)\n",
    "\n",
    "    if glCheckFramebufferStatus(GL_FRAMEBUFFER) != GL_FRAMEBUFFER_COMPLETE:\n",
    "        print(\"ERROR::FRAMEBUFFER:: Framebuffer is not complete!\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Framebuffer is complete!\")\n",
    "        \n",
    "    return program, sphereTexture, sphereLRTexture, VAO, framebuffer, lrframebuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv resize time:  0.058817148208618164\n"
     ]
    }
   ],
   "source": [
    "erp_height, erp_width = 2560, 5120\n",
    "new_frame = cv.imread('Eagle_result/first_frame_5120x2560.png')\n",
    "# new_frame = cv.flip(new_frame, 0)\n",
    "# new_frame = cv.cvtColor(new_frame, cv.COLOR_BGR2RGBA)\n",
    "startTime = time.time()\n",
    "new_frame_LR = cv.resize(new_frame ,(erp_width//4, erp_height//4), interpolation=cv.INTER_AREA)\n",
    "cv.imwrite('Eagle_result/first_frame_1280x720_AREA_CV.png', new_frame_LR)\n",
    "print(\"cv resize time: \", time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main\n",
    "1. 4K ERP 이미지가 INPUT\n",
    "2. 4K ERP -> 6 Tiles perspective 이미지로 변환(HR)\n",
    "3. 1/4 size ERP -> 6 Tiles perspective 이미지로 변환(LR)\n",
    "4. 6 Tiles x4 SR(SR)\n",
    "5. 6 Tiles VMAF score 측정 (HR <-> SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] device cuda\n",
      "{'block': [4, {'0': ['Dense', {'block_inference_type': 'PerFrame', 'bottleneck_channel': 72, 'channel_attention': 0, 'layer_depth': 4, 'upsampling_channels': 8, 'upsampling_type': 'Sub'}], '1': ['Dense', {'block_inference_type': 'PerFrame', 'bottleneck_channel': 72, 'channel_attention': 0, 'layer_depth': 4, 'upsampling_channels': 8, 'upsampling_type': 'Sub'}], '2': ['Dense', {'block_inference_type': 'PerFrame', 'bottleneck_channel': 72, 'channel_attention': 0, 'layer_depth': 4, 'upsampling_channels': 8, 'upsampling_type': 'Sub'}], '3': ['Dense', {'block_inference_type': 'PerFrame', 'bottleneck_channel': 72, 'channel_attention': 0, 'layer_depth': 4, 'upsampling_channels': 8, 'upsampling_type': 'Sub'}]}], 'block_channels': 12, 'arch_name': 'x4_Sub_12_exit4_Dense_fixed_72_0_4', 'scale_factor': 4, 'upsampling_channels': 4, 'upsampling_type': 'Sub'}\n",
      "[DEBUG, owl_model_v1] Reconstructors SubPixelUpscaleModuleY(\n",
      "  (deconv): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      ")\n",
      "SPHERE INIT\n",
      "Framebuffer is complete!\n",
      "cv resize time:  0.007815361022949219\n",
      "HR tiles image saved, time:  0.8288257122039795\n",
      "LR tiles image saved, time:  0.01249074935913086\n",
      "[DEBUG] torch inference time:  5.176067352294922\n",
      "[DEBUG] cpu detach time 41.312456130981445\n",
      "[DEBUG] video write time per frame:  111.86981201171875\n",
      "[DEBUG] video write time per frame:  201.6775608062744\n",
      "[DEBUG] video write time per frame:  293.15638542175293\n",
      "[DEBUG] video write time per frame:  376.0976791381836\n",
      "[DEBUG] video write time per frame:  464.0359878540039\n",
      "[DEBUG] video write time per frame:  549.633264541626\n",
      "[DEBUG] video write time per frame:  631.9682598114014\n",
      "[DEBUG] video write time per frame:  753.2353401184082\n",
      "inference time:  0.803027868270874\n",
      "vmaf calculate time:  0.35372161865234375\n",
      "vmaf calculate time:  1.12143874168396\n",
      "vmaf calculate time:  1.9148690700531006\n",
      "vmaf calculate time:  2.7135467529296875\n",
      "vmaf calculate time:  3.4775049686431885\n",
      "vmaf calculate time:  4.189311265945435\n",
      "vmaf calculate time:  5.01682448387146\n",
      "vmaf calculate time:  5.84290075302124\n",
      "vmaf_scores:  [61.897528, 70.438081, 63.563012, 58.823023, 44.809683, 55.361874, 69.59546, 56.411321]\n",
      "vmaf_scores_LR:  [42.262849, 47.625373, 37.794461, 33.672796, 23.311645, 35.156449, 48.568027, 36.957913]\n",
      "vmaf_diff:  [19.634679 22.812708 25.768551 25.150227 21.498038 20.205425 21.027433\n",
      " 19.453408]\n",
      "vmaf overall time:  6.213879108428955\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # parser = argparse.ArgumentParser(\n",
    "    #     description=\"Video super resolution TEST using Pytorch\",\n",
    "    # )\n",
    "    # parser.add_argument(\"-i\", help=\"input path\", default=\"Eagle_result/first_frame_5120x2560.png\")\n",
    "    # # parser.add_argument(\"-o\", help=\"output path\", default=\"torch_result.mp4\")\n",
    "    # parser.add_argument(\"-gpu\", help=\"gpu device num\", default=\"7\")\n",
    "    # parser.add_argument(\"-exit_num\", help=\"model exit num 1~5\", default=\"5\")\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    env[\"CUDA_VISIBLE_DEVICES\"] = '4'\n",
    "    ## HR 기준 width, height\n",
    "    erp_height, erp_width = 2560, 5120 # LR의 경우 720, 1280\n",
    "    view_height, view_width = 720, 1280 # LR의 경우 180, 320\n",
    "    view_height, view_width = 1280, 1280 # 8개 정사각형 tile로 나눌경우\n",
    "    sr_scale = 4\n",
    "    # exit_num = args.exit_num\n",
    "    \n",
    "    model, device = sr_init(exit_num=1)\n",
    "    lr_frames = []\n",
    "    \n",
    "    # create opengl context\n",
    "    display, egl_context = create_opengl_context()\n",
    "    # create sphere\n",
    "    sphere = Sphere()\n",
    "    # create camera\n",
    "    camera = Camera()\n",
    "    \n",
    "    views = [(45,45), (135,45), (225,45), (315,45), (45,-45), (135,-45), (225,-45), (315,-45)]\n",
    "    # views = [(0,0), (90,0), (180,0), (270,0), (0,90), (0,-90)]\n",
    "    directions = []\n",
    "    for jaw, pitch in views:\n",
    "        camera.update_camera_vectors(pitch, jaw)\n",
    "        directions.append((jaw, pitch, camera.get_view_matrix()))\n",
    "        \n",
    "    # init opengl\n",
    "    program, sphereTexture, sphereLRTexture, VAO, framebuffer, lrframebuffer  = opengl_init(sphere, view_width, view_height, erp_width, erp_height)\n",
    "        \n",
    "    glUseProgram(program)\n",
    "    glClearColor(0., 0.25, 0.25, 1)\n",
    "    \n",
    "    # load image\n",
    "    # new_frame = cv.imread(args.i)\n",
    "    new_frame = cv.imread('Eagle_result/first_frame_5120x2560.png')\n",
    "    new_frame = cv.flip(new_frame, 0)\n",
    "    new_frame = cv.cvtColor(new_frame, cv.COLOR_BGR2RGBA)\n",
    "    startTime = time.time()\n",
    "    new_frame_LR = cv.resize(new_frame ,(erp_width//4, erp_height//4), interpolation=cv.INTER_AREA)\n",
    "    print(\"cv resize time: \", time.time() - startTime)\n",
    "\n",
    "    glBindVertexArray(VAO)\n",
    "\n",
    "    image_data = np.zeros((view_height, view_width, 3), dtype=np.uint8)\n",
    "    LR_image_data = np.zeros((view_height//4, view_width//4, 3), dtype=np.uint8)\n",
    "    \n",
    "    startTime = time.time()\n",
    "\n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, framebuffer)\n",
    "    glBindTexture(GL_TEXTURE_2D, sphereTexture)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, new_frame.shape[1], new_frame.shape[0], 0, GL_RGBA, GL_UNSIGNED_BYTE, new_frame.data)\n",
    "    \n",
    "    init_viewport(program, view_width, view_height)\n",
    "    ### HR rendering and save ###\n",
    "    for jaw, pitch, view in directions:\n",
    "        view_change(program, view)\n",
    "        #### HR image save ####\n",
    "        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "        \n",
    "        glDrawElements(GL_TRIANGLES, sphere.indexCount, GL_UNSIGNED_SHORT, None)\n",
    "\n",
    "        glReadPixels(0, 0, view_width, view_height, GL_RGB, GL_UNSIGNED_BYTE, image_data)\n",
    "\n",
    "        image_data = cv.cvtColor(image_data, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # 해상도가 커서 저장이 오래걸림.. 저장하는거 빼면 0.1초 미만\n",
    "        cv.imwrite(f'Eagle_result/test_sphere_view_({jaw},{pitch}).png', image_data)\n",
    "    print(\"HR tiles image saved, time: \", time.time() - startTime)\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, lrframebuffer)\n",
    "    glBindTexture(GL_TEXTURE_2D, sphereLRTexture)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, new_frame_LR.shape[1], new_frame_LR.shape[0], 0, GL_RGBA, GL_UNSIGNED_BYTE, new_frame_LR.data)\n",
    "    \n",
    "    init_viewport(program, view_width//4, view_height//4)\n",
    "    ### LR rendering and save ###\n",
    "    for jaw, pitch, view in directions:\n",
    "        view_change(program, view)\n",
    "        #### LR image save ####\n",
    "        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "\n",
    "        glDrawElements(GL_TRIANGLES, sphere.indexCount, GL_UNSIGNED_SHORT, None)\n",
    "\n",
    "        glReadPixels(0, 0, view_width//4, view_height//4, GL_RGB, GL_UNSIGNED_BYTE, LR_image_data)\n",
    "\n",
    "        lr_frames.append(cv.cvtColor(LR_image_data, cv.COLOR_RGB2YUV_I420))\n",
    "        # LR_image_data = cv.cvtColor(LR_image_data, cv.COLOR_RGB2BGR)\n",
    "\n",
    "        # cv.imwrite(f'Eagle_result/test_sphere_view_LR_({jaw},{pitch}).png', LR_image_data)\n",
    "    print(\"LR tiles image saved, time: \", time.time() - startTime)\n",
    "\n",
    "    #### inference process ####\n",
    "    startTime = time.time()\n",
    "    inference(model, device, lr_frames, sr_scale)\n",
    "    print(\"inference time: \", time.time() - startTime)\n",
    "    \n",
    "    #### VMAF calculation ####\n",
    "    startTime = time.time()\n",
    "    i = 0\n",
    "    vmaf_scores = []\n",
    "    vmaf_scores_lr = []\n",
    "    for jaw, pitch, _ in directions:\n",
    "        ref = f'/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/test_sphere_view_({jaw},{pitch}).png'\n",
    "        dis = f'/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/test_sphere_view_SR_({i}).png'\n",
    "        i += 1\n",
    "        json_result = calculate_vmaf(ref, dis)\n",
    "        vmaf_scores.append(json_result['vmaf_mean'])\n",
    "        print('vmaf calculate time: ', time.time() - startTime)\n",
    "        \n",
    "        dis2 = f'/data/home/mckim/Eagle/VMAF-Tester/Eagle_result/test_sphere_view_LR_({jaw},{pitch}).png'\n",
    "        json_result = calculate_vmaf(ref, dis2)\n",
    "        vmaf_scores_lr.append(json_result['vmaf_mean'])\n",
    "\n",
    "    print(\"vmaf_scores: \", vmaf_scores)\n",
    "    print(\"vmaf_scores_LR: \", vmaf_scores_lr)\n",
    "    print(\"vmaf_diff: \", np.array(vmaf_scores) - np.array(vmaf_scores_lr))\n",
    "    print(\"vmaf overall time: \", time.time() - startTime)\n",
    "\n",
    "    glBindFramebuffer(GL_FRAMEBUFFER, 0)\n",
    "    glBindVertexArray(0)\n",
    "    \n",
    "    # destroy opengl context\n",
    "    destroy_opengl_context(display, egl_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Eagle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
